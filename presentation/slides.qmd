---
title: A Long-Term Archival Pipeline for the Forschungsdatenplattform Stadt.Geschichte.Basel
subtitle: "[omeka2dsp](https://dokumentation.stadtgeschichtebasel.ch/omeka2dsp/)"
author:
  - name: "Moritz Mähr"
    affiliation:
    - "University of Basel"
    - "University of Bern"
    orcid: "0000-0002-1367-1618"
    email: "moritz.maehr@gmail.com"
  - name: "Moritz Twente"
    affiliation: "University of Basel"
    orcid: "0009-0005-7187-9774"
    email: "mtwente@protonmail.com"
date: 2025-10-15
abstract: |
  The Forschungsdatenplattform by Stadt.Geschichte.Basel provides open access to diverse historical materials relating to the city of Basel, including texts, images, statistical, and geospatial data. While technically robust and publicly available through GitHub Pages using the CollectionBuilder-CSV, our current infrastructure faces critical sustainability and scalability limitations.

  At present, metadata and files are curated and managed using the Omeka-S instance provided by the University of Bern, from which we extract the data for display via CollectionBuilder. However, this setup introduces significant risks for long-term availability:

  - Omeka is not under our institutional control and may not remain permanently funded.

  - GitHub Pages is not suitable for serving large files or guaranteeing persistence.

  - CollectionBuilder lacks built-in support for versioning and persistent identifiers at the level required by research data infrastructures.

  To address these challenges, we are implementing a transition pipeline to archive all metadata and associated files with DaSCH, leveraging its infrastructure for versioned, durable, and FAIR-compliant research data publication. The new pipeline includes:

  1. Metadata harvesting and transformation from Omeka to the DaSCH data model.

  2. Automated deposit and update routines using the DaSCH REST API, including support for versioning existing records.

  3. Writing back stable DaSCH identifiers and access URLs to our public-facing platform, ensuring transparency and citability.

  4. Preservation of hierarchical relationships and media metadata, aligned with our custom metadata model, which incorporates principles of anti-discriminatory description practices.

  This transition allows us to decouple the archival backend from the front-end presentation, ensuring long-term data accessibility, citability, and semantic interoperability. In our presentation, we will share the architectural overview, code-level considerations, and our reflections on working with DaSCH APIs in a real-world context, including:

  - Lessons learned in metadata crosswalks and transformation logic.

  - Technical caveats in version control, file transfers, and identifier management.

  - Challenges in aligning minimal computing approaches (CollectionBuilder) with robust backend infrastructures (DSP).

  This case study illustrates how lightweight digital publishing environments can be effectively combined with national research infrastructures to deliver sustainable, standards-based access to historical research data. It also raises broader questions about infrastructural independence, scalability of humanities platforms, and the practical challenges of implementing FAIR principles in community-developed software ecosystems.
format:
  revealjs:
    theme: simple
    css: /styles.css
    slide-number: true
    incremental: false
    # transition: slide
    code-line-numbers: true
    embed-resources: true
    menu:
      side: right
      width: normal
    height: 900
    width: 1600
    preview-links: auto
    auto-stretch: true
    fig-align: center
  pptx:
    reference-doc: "SGB_PowerPoint_Vorlage.potx"
---

## ![](./images/book_series_cover.png){fig-align="center"}

-   Large-scale historical research project, initiated in 2011 by the Association for Basel History and carried out 2017–2026 at the University of Basel
-   More than 70 researchers studying the history of Basel from the earliest settlements to the present day
-   Funded with more than 9 million Swiss francs by the Canton of Basel-Stadt, the Lottery Fund, and private sponsors

. . .

-   Specialized Team for Research Data Management and Public History
-   Various research outputs   (incl. books, papers, data stories, figures, and source code)

## Research Data

```{mermaid}
flowchart LR
  subgraph Research[Research]
    Publications[📚 Publications]
    Data[📊 Statistical & Geo Data]
    Code[💻 Source Code]
  end
```

## Collecting and Managing Research Data

```{mermaid}
flowchart LR
  subgraph Research[Research]
    Publications[📚 Publications]
    Data[📊 Statistical & Geo Data]
    Code[💻 Source Code]
  end

  subgraph Repositories
    Omeka[(📁 omeka.unibe.ch)]
    GitHub[(🐙 GitHub)]
  end

  %% Flows
  Publications -- figures --> Omeka
  Data -- visualizations --> Omeka
  Code --> GitHub
```

## Public History with Research Data

```{mermaid}
flowchart LR
  subgraph Research[Research]
    Publications[📚 Publications]
    Data[📊 Statistical & Geo Data]
    Code[💻 Source Code]
  end

  subgraph Repositories[Repositories]
    Omeka[(📁 omeka.unibe.ch)]
    GitHub[(🐙 GitHub)]
  end

  subgraph PublicWeb[Public History Websites]
    RDP[forschung.stadtgeschichtebasel.ch]
  end

  %% Flows
  Publications -- figures --> Omeka
  Data -- visualizations --> Omeka
  Code --> GitHub
  Omeka -- API --> RDP
  GitHub -- static site generator --> RDP
```

## Archiving Research Data for the Long Run

```{mermaid}
flowchart LR
  subgraph Research[Research]
    Publications[📚 Publications]
    Data[📊 Statistical & Geo Data]
    Code[💻 Source Code]
  end

  subgraph Repositories[Repositories]
    Omeka[(📁 omeka.unibe.ch)]
    GitHub[(🐙 GitHub)]
  end

  subgraph PublicWeb[Public History Websites]
    RDP[Research Data Platform<br>forschung.stadtgeschichtebasel.ch]
  end

  subgraph Archives[Long-term Archives]
    Zenodo[(📦 Zenodo)]
    DaSCH[(🏛️ DaSCH)]
    UBBasel[(📚 University Library Basel)]
  end

  %% Flows
  Publications -- books --> UBBasel
  Publications ~~~ UBBasel
  Publications -- other publications --> Zenodo
  Publications -- figures --> Omeka
  Data -- visualizations --> Omeka
  Omeka --> DaSCH
  Omeka -- API --> RDP
  Code --> GitHub
  GitHub --> Zenodo
  GitHub -- Static Site Generator --> RDP
```

## Long-Term Preservation

-   **Institutional sustainability**: External dependencies (Omeka at UniBern) may not be permanently funded
-   **Scalability**: Current minimal computing approach (GitHub Pages) cannot handle large files

## Where DaSCH Fits in: omeka2dsp
![](./images/mermaid/ecosystem_5_dasch.png){fig-align="center"}

```{mermaid}
flowchart LR
  subgraph Research[Research]
    Publications[📚 Publications]
    Data[📊 Statistical & Geo Data]
  end

  subgraph Repositories
    Omeka[(📁 omeka.unibe.ch)]
  end

  subgraph Archives[Long-term Archives]
    DaSCH[(🏛️ DaSCH)]
  end

  %% Flows
  Publications -- figures --> Omeka
  Data -- visualizations --> Omeka
  Omeka --> DaSCH
```

## Challenges

- Data model differences (Omeka vs DaSCH)
- Metadata transformation and crosswalks
-   Data model differences (Omeka vs DaSCH)
-   Metadata transformation and crosswalks
-   Automation of deposit and version control

#  Data model differences

# Omeka

# {data-background-image="./images/omeka-collection.png"}

# {data-background-image="./images/omeka-item.png"}

# {data-background-image="./images/omeka-medium_1.png"}

# {data-background-image="./images/omeka-medium_2.png"}

##  Data model Omeka (simplified 😇)

```{mermaid}
classDiagram
  %% Core Omeka S entities (reduced)
  class Item {
    o:id : int
    o:is_public : bool
    o:title : string
    dcterms:identifier : string
    dcterms:subject[*] : IconclassTerm
    dcterms:temporal : Era
    dcterms:language : ISO639
    o:created : datetime
    o:modified : datetime
  }

  class Media {
    o:id : int
    o:item_id : int
    o:ingester : string
    o:media_type : MIME
    o:original_url : uri
    o:sha256 : hash
    dcterms:creator[*] : uri|text
    dcterms:date : string~EDTF
    dcterms:license : LicenseURI
    dcterms:rights : text
  }

  class ItemSet {
    o:id : int
    o:label : string
  }

  %% Controlled vocabularies as types
  class Era { <<type>> }
  class MIME { <<type>> }
  class LicenseURI { <<type>> }
  class IconclassTerm {
    <<external scheme>>
    code : string
    label : string
  }
  class ISO639 {
    <<code>>
    value : string
  }

  %% Relations and cardinalities
  Item "1" o-- "0..*" Media : has media
  Media "*" --> "1" Item : belongs to
  Item "*" o-- "0..*" ItemSet : in set(s)
  Item --> "0..*" IconclassTerm : subjects
  Media --> "0..*" IconclassTerm : subjects
  Item --> "1" Era : temporal
  Media --> "1" Era : temporal
  Media --> MIME : media_type
  Media --> LicenseURI : license
  Item --> ISO639 : language
```

## Data model DaSCH (simplified 😅)

::: {layout="[[-1], [1], [-1]]"}

```{mermaid}
classDiagram
class Parent
class Document
class ResourceWithoutMedia
class Image

class SubjectList
class LanguageList
class TypeList
class FormatList
class TemporalList
class LicenseList

%% Core links to Parent object
Document "0..1" --> "1" Parent : linkToParentObject
ResourceWithoutMedia "0..1" --> "1" Parent : linkToParentObject
Image "0..1" --> "1" Parent : linkToParentObject

%% Value-list relations
Parent "1" --> "1" TemporalList : hasTemporalList
Parent "1" --> "0..*" SubjectList : hasSubjectList
Parent "1" --> "0..1" LanguageList : hasLanguageList

Document "1" --> "0..1" SubjectList : hasSubjectList
Document "1" --> "0..1" TemporalList : hasTemporalList
Document "1" --> "0..1" TypeList : hasTypeList
Document "1" --> "0..1" FormatList : hasFormatList
Document "1" --> "0..1" LanguageList : hasLanguageList
Document "1" --> "0..1" LicenseList : hasLicenseList

ResourceWithoutMedia "1" --> "0..1" SubjectList : hasSubjectList
ResourceWithoutMedia "1" --> "0..1" TemporalList : hasTemporalList
ResourceWithoutMedia "1" --> "0..1" TypeList : hasTypeList
ResourceWithoutMedia "1" --> "0..1" FormatList : hasFormatList
ResourceWithoutMedia "1" --> "0..1" LanguageList : hasLanguageList
ResourceWithoutMedia "1" --> "0..1" LicenseList : hasLicenseList

Image "1" --> "0..1" SubjectList : hasSubjectList
Image "1" --> "0..1" TemporalList : hasTemporalList
Image "1" --> "0..1" TypeList : hasTypeList
Image "1" --> "0..1" FormatList : hasFormatList
Image "1" --> "0..1" LanguageList : hasLanguageList
Image "1" --> "0..1" LicenseList : hasLicenseList
```

## Key differences

- **Modeling approach:**

  - *DaSCH*: Class hierarchy (`Resource` → `Document` / `Image` …), explicit value classes (`TextValue`, `ListValue`).
  - *Omeka S*: Flat JSON-LD model (`Item`, `Media`, `ItemSet`), Dublin Core–centric.
    -   *DaSCH*: Class hierarchy (`Resource` → `Document` / `Image` …), explicit value classes (`TextValue`, `ListValue`).
    -   *Omeka S*: Flat JSON-LD model (`Item`, `Media`, `ItemSet`), Dublin Core–centric.

- **Normalization & constraints:**

  - *DaSCH*: Strict cardinalities and mandatory fields (`hasTitle [1]`).
  - *Omeka S*: More flexible, "validation" through templates.
    -   *DaSCH*: Strict cardinalities and mandatory fields (`hasTitle [1]`).
    -   *Omeka S*: More flexible, "validation" through templates.

- **Hierarchy representation:**

  - *DaSCH*: Explicit `Parent` class and `linkToParentObject`.
  - *Omeka S*: Uses field `ItemSet` or `Media` to model relations.
    -   *DaSCH*: Explicit `Parent` class and `linkToParentObject`.
    -   *Omeka S*: Uses field `ItemSet` or `Media` to model relations.

# Metadata transformation

## Data validation

- Custom Python scripts using `pydantic` for schema validation as Omeka does not enforce strict validation
- Validation also helps identify data quality issues
-   Custom Python scripts using `pydantic` for schema validation as Omeka does not enforce strict validation
-   Validation also helps identify data quality issues
-   Lots of manual cleaning required, all done in Omeka

## Crosswalk Examples

| Omeka Property | DaSCH Property | Notes |
|------------------------|------------------------|------------------------|
| `dcterms:title` | `hasTitle` | Required in DaSCH (cardinality 1) |
| `dcterms:identifier` | `hasIdentifier` | Not by DSP used for stable references |
| `dcterms:subject` | `hasSubjectList` | Iconclass codes preserved |
| `dcterms:temporal` | `hasTemporalList` | Era mapping required |
| `dcterms:creator` | `hasCreatorList` | Multiple creators supported |
| `dcterms:license` | `hasLicenseList` | License URIs validated |
| Media → Item | `linkToParentObject` | Hierarchy explicitly modeled |

## Version control and updates

- **Challenge**: DaSCH supports versioning, but requires careful planning
- **Strategy**:
-   **Challenge**: DaSCH supports versioning, but requires careful planning
-   **Strategy**:
    -   Initial deposit: Create new resources via REST API
    -   Updates: Use PUT requests with resource IDs to create new versions
    -   Identifier stability: ARK IDs remain constant across versions

## API vs DSP-Tools

::::: columns
::: {.column width="50%"}

### REST API Approach

- Direct HTTP requests
- Fine-grained control
-   Direct HTTP requests
-   Fine-grained control
-   Supports versioning
-   Complex error handling
-   Used for updates
:::

::: {.column width="50%"}

### DSP-Tools Approach

- XML-based bulk import
- Good for initial deposits
-   XML-based bulk import
-   Good for initial deposits
-   Less flexible for updates
-   Comprehensive validation
-   Used for large-scale ingestion
:::
:::::

**Our choice**: Using dsp-tools for project setup and Rest API for ingestions/updates

# omeka2dsp

# {data-background-image="./images/omeka2dsp_overview.png"}

# {data-background-image="./images/omeka2dsp_system_architecture.png"}

# {data-background-image="./images/omeka2dsp_data_workflow.png"}

## Lessons Learned

- **Metadata crosswalks and data quality**: Mapping Dublin Core to the DaSCH ontology required custom logic and validation. Omeka’s flexible schema led to inconsistent metadata, demanding extensive cleaning.
- **Identifiers and file handling**: Synchronizing identifiers between Omeka and DaSCH proved complex. Large files (>100 MB) necessitated chunked uploads and tailored handling.
-   **Metadata crosswalks and data quality**: Mapping Dublin Core to the DaSCH ontology required custom logic and validation. Omeka’s flexible schema led to inconsistent metadata, demanding extensive cleaning.
-   **Identifiers and file handling**: Synchronizing identifiers between Omeka and DaSCH proved complex. Large files (\>100 MB) necessitated chunked uploads and tailored handling.
-   **Workflow timing and validation**: Determining the right moment to shift from active curation to archival mode was key. Early validation with `pydantic` schemas and subset testing prevented costly errors.
-   **Documentation and reproducibility**: Precise mapping documentation ensured consistency across transformations and supported reproducible workflows.
-   **Architecture and infrastructure**: Decoupling archival (DaSCH) from presentation (CollectionBuilder) enhanced flexibility. Lightweight public interfaces can coexist with robust preservation systems when APIs enable automation.

## Key Takeaways

::::: columns
::: {.column width="50%"}

### For the Community

- Lightweight publishing can work with robust infrastructure
- FAIR principles are achievable in practice
-   Lightweight publishing can work with robust infrastructure
-   FAIR principles are achievable in practice
-   Decoupled architectures provide flexibility
-   Open-source tools enable customization
:::

::: {.column width="50%"}

### For DaSCH Users

- Plan metadata transformation early
- Use validation extensively
-   Plan metadata transformation early
-   Use validation extensively
-   Consider hybrid API/DSP-Tools approach
-   Document mapping decisions thoroughly
:::
:::::

## Resources

- [Research Data Platform](https://forschung.stadtgeschichtebasel.ch)
- [omeka2dsp Documentation](https://dokumentation.stadtgeschichtebasel.ch/omeka2dsp)
- [Documentation Platform](https://dokumentation.stadtgeschichtebasel.ch)
